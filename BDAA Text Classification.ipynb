{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://bdaaosu.org/img/Logo.png\" width=\"60%\"/>\n",
    "\n",
    "# Text Classification Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def most_associated_words(tfidf, features, labels):\n",
    "    from sklearn.feature_selection import chi2\n",
    "    import numpy as np\n",
    "    N = 5\n",
    "    for cond in list(set(labels)):\n",
    "      features_chi2 = chi2(features, labels == cond)\n",
    "      indices = np.argsort(features_chi2[0])\n",
    "      feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "      unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "      bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "      #trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "      print(\"# '{}':\".format(cond))\n",
    "      print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "      print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
    "      #print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How important is a word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Two example \"documents\"\n",
    "example = [\n",
    "    'Perfectly cooked and seasoned',\n",
    "    'Illustrious and a symbol of strength'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import the \"TfidfVectorizer\" from scikit-learn\n",
    "import pandas as pd # Allows us to view data as a nicely formatted table!\n",
    "\n",
    "# Make a TF-IDF vectorizer object\n",
    "tfidf = TfidfVectorizer( \n",
    "    lowercase=True, # Make every word in our documents lowercase\n",
    "    stop_words='english' # Remove common words like \"a\", \"is\", \"they\", \"with\", etc.\n",
    ")\n",
    "\n",
    "# Transform the \"documents\" we have to a matrix of TF-IDF values\n",
    "features = tfidf.fit_transform(example).toarray()\n",
    "\n",
    "print(\n",
    "    # Follow along!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's put on our math hats\n",
    "\n",
    "Why does the word \"illustrious\" have a TF-IDF value of 0.57735?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5643823935199818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_documents = 2\n",
    "num_words = 3\n",
    "term_frequency = 1/num_words\n",
    "inverse_document_frequency = np.log(num_documents/1)+1\n",
    "\n",
    "tf_idf = (1/3)*(np.log(2)+1)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Okay, _cool math dude_, so now we have a huge matrix. \n",
    "### So, how do we use it to make classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What the f@!k is Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter Bayes Rule\n",
    "<img src=\"https://miro.medium.com/max/512/0*EfYTXtTJ9X-Ua9Nh.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From our first (small) example\n",
    "<img src=\"https://i.imgur.com/WnKCeD3.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Food or not food?\n",
    "<img src=\"https://i.imgur.com/a83Evsd.png\" width=\"55%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to need some training data...\n",
    "\n",
    "## Why not classify some listings together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/qXcsZPi.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><h2>go.osu.edu/bdaa_ctc</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's do some classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the classifications we came up with!\n",
    "descriptions = pd.read_csv('https://bdaa-text-workshop.s3.amazonaws.com/iPhone+Listing+Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# What's the distribution of the condition classes we came up with?\n",
    "# Follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=.05, \n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2), # Consider both one word and two word combinations\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "features = tfidf.fit_transform(descriptions.ItemDescription).toarray()\n",
    "labels = descriptions.Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get most associated words with each condition category \n",
    "# Follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train, train, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Split a dataset into training and test datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Make a matrix of word counts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Transform a matrix of word counts into TF-IDF values\n",
    "from sklearn.naive_bayes import MultinomialNB # Make a Multinomial Naive Bayes model\n",
    "\n",
    "# Make training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    descriptions['ItemDescription'], \n",
    "    descriptions['Condition'], \n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "# Transform our training data into word counts\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "# ..and then TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train the Multinomial Naive Bayes model!\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classify the real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We need some value data!\n",
    "iphone_listings = pd.read_csv('https://bdaa-text-workshop.s3.amazonaws.com/eBay+iPhone+Listings.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "# Follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    # Use the model we trained to predict labels!\n",
    "    clf.predict(\n",
    "        # Transform the listing descriptions into a matrix of counts\n",
    "        count_vect.transform(\n",
    "            iphone_listings.head(100).ItemDescription.tolist()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new column in the real data with the predicted Condition\n",
    "iphone_listings['Condition'] = clf.predict(\n",
    "        # Transform the listing descriptions into a matrix of counts\n",
    "        count_vect.transform(\n",
    "            iphone_listings.ItemDescription.tolist()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each class did we predict?\n",
    "# Follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who'se got a used iPhone?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
